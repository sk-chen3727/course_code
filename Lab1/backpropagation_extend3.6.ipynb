{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6chybAVFJW2",
        "tags": []
      },
      "source": [
        "# **Notebook: Backpropagation**\n",
        "\n",
        "This notebook runs the backpropagation algorithm on a deep neural network as described in section 7.4 of the book for the least square loss function and for one data sample. In the end, it compares the derivatives with finite difference.\n",
        "\n",
        "After working though this notebook, you will use this notebook to extend to handle softmax output and cross-entropy loss as well as multiple data points.\n",
        "\n",
        "The notebook is a slight modification of https://github.com/udlbook/udlbook/blob/main/Notebooks/Chap07/7_2_Backpropagation.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LdIDglk1FFcG",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnUoI0m6GyjC",
        "tags": []
      },
      "source": [
        "First let's define a neural network.  We'll just choose the weights and biases randomly for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WVM4Tc_jGI0Q",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Set seed so we always get the same random numbers\n",
        "np.random.seed(0)\n",
        "\n",
        "# Number of hidden layers\n",
        "K = 5\n",
        "# Number of neurons per layer\n",
        "D = 6\n",
        "# Input layer dimension\n",
        "D_i = 1\n",
        "# Output layer dimension\n",
        "D_o = 3\n",
        "\n",
        "# Make empty lists\n",
        "all_weights = [None] * (K+1)\n",
        "all_biases = [None] * (K+1)\n",
        "\n",
        "# Create input and output layer\n",
        "if K==0:\n",
        "    all_weights[0] = np.random.normal(size=(D_o, D_i))\n",
        "else:\n",
        "    all_weights[0] = np.random.normal(size=(D, D_i)) # input\n",
        "    all_weights[-1] = np.random.normal(size=(D_o, D)) # output\n",
        "    all_biases[0] = np.random.normal(size =(D,1)) # input bias\n",
        "\n",
        "all_biases[-1]= np.random.normal(size =(D_o,1)) # output bias\n",
        "\n",
        "# Create intermediate layers\n",
        "for layer in range(1,K):\n",
        "    all_weights[layer] = np.random.normal(size=(D,D))\n",
        "    all_biases[layer] = np.random.normal(size=(D,1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(f):\n",
        "    # f: shape (n_samples, D_o)\n",
        "    f_exp = np.exp(f - np.max(f, axis=1, keepdims=True))  # Avoiding Numerical Overflow\n",
        "    return f_exp / np.sum(f_exp, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "2J3qK5JyvfEz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jZh-7bPXIDq4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Define the Rectified Linear Unit (ReLU) function\n",
        "def ReLU(preactivation):\n",
        "    activation = preactivation.clip(0.0)\n",
        "    return activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5irtyxnLJSGX",
        "tags": []
      },
      "source": [
        "Now let's run our random network.  The weight matrices $\\boldsymbol\\Omega_{1\\ldots K}$ are the entries of the list \"all_weights\" and the biases $\\boldsymbol\\beta_{1\\ldots K}$ are the entries of the list \"all_biases\".\n",
        "\n",
        "We know that we will need the preactivations $\\mathbf{f}_{0\\ldots K}$ and the activations $\\mathbf{h}_{1\\ldots K}$ later for the backward pass of backpropagation, so we'll store and return these as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LgquJUJvJPaN",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def forward_pass(net_input, all_weights, all_biases):\n",
        "\n",
        "    # Retrieve number of layers\n",
        "    K = len(all_weights) -1\n",
        "\n",
        "    # We'll store the pre-activations at each layer in a list \"all_f\"\n",
        "    # and the activations in a second list \"all_h\".\n",
        "    all_f = [None] * (K+1)\n",
        "    all_h = [None] * (K+1)\n",
        "\n",
        "    #For convenience, we'll set all_h[0] to be the input, and all_f[K] will be the output\n",
        "    all_h[0] = net_input\n",
        "\n",
        "    # Run through the layers, calculating all_f[0...K-1] and all_h[1...K]\n",
        "    for layer in range(K):\n",
        "        # Update preactivations and activations at this layer according to eq. 7.16 (eq. 7.17 if you use pdf version)\n",
        "        # Remember to use np.matmul for matrix multiplications\n",
        "        # TODO -- Replace the lines below\n",
        "        # all_f[layer] = all_h[layer]\n",
        "        # all_h[layer+1] = all_f[layer]\n",
        "\n",
        "        all_f[layer] = all_h[layer] @ all_weights[layer].T + all_biases[layer].T\n",
        "        # all_weights(D, D_i) all_biases[0] (D,1) net_input n_samples,D_i)\n",
        "        all_h[layer+1] = ReLU(all_f[layer])\n",
        "\n",
        "    # Compute the output from the last hidden layer\n",
        "    # TODO -- Replace the line below\n",
        "    # all_f[K] = np.zeros_like(all_biases[-1])\n",
        "    # all_f[K] = all_biases[-1] + all_h[K] @ all_weights[-1].T\n",
        "    all_f[K] = all_h[K] @ all_weights[-1].T + all_biases[-1].T\n",
        "\n",
        "    # Retrieve the output\n",
        "    net_output = all_f[K]\n",
        "\n",
        "    return net_output, all_f, all_h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IN6w5m2ZOhnB",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaa6b6df-039b-4cb2-d526-5d74b04e3d88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True output = 2.757, Your answer = 4.771\n"
          ]
        }
      ],
      "source": [
        "# Define input\n",
        "n_samples= 10 # Number of samples\n",
        "np.random.seed(0)\n",
        "net_input = np.random.normal(0,1,size=(D_i,n_samples)) # If data points as columns\n",
        "net_input = net_input.T\n",
        "\n",
        "# Compute network output\n",
        "net_output, all_f, all_h = forward_pass(net_input,all_weights, all_biases)\n",
        "print(\"True output = %3.3f, Your answer = %3.3f\"%(2.757, net_output[0,0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxVTKp3IcoBF",
        "tags": []
      },
      "source": [
        "Now let's define a cost function. We'll just use the least squares loss function for this. We'll also write a function to compute d_cost_d_output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lByyc2ok-Ndn",
        "outputId": "d0057905-7298-4b6d-c4fd-00e8466254ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y =  [[0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]]\n",
            "Cost = 1.413\n"
          ]
        }
      ],
      "source": [
        "# def compute_cost(net_output, y):\n",
        "#     I = y.shape[0]  # Number of data points if data points as columns\n",
        "#     return np.sum((net_output-y) * (net_output-y))/I\n",
        "\n",
        "def compute_cost(net_output, y):\n",
        "    \"\"\"\n",
        "    net_output: (n_samples, D_o) logits\n",
        "    y: (n_samples, D_o) one-hot labels\n",
        "    \"\"\"\n",
        "    f = net_output\n",
        "    return np.mean(np.log(np.sum(np.exp(f - np.max(f, axis=1, keepdims=True)), axis=1)) - np.sum(y * f, axis=1))\n",
        "\n",
        "\n",
        "# def d_cost_d_output(net_output, y):\n",
        "#     I = y.shape[0] # Number of data points if data points as columns\n",
        "#     return 2*(net_output -y)/I\n",
        "\n",
        "def d_cost_d_output(net_output, y):\n",
        "    \"\"\"\n",
        "    net_output: (n_samples, D_o)\n",
        "    y: (n_samples, D_o)\n",
        "    \"\"\"\n",
        "    return softmax(net_output) - y\n",
        "\n",
        "# Define the output we later will use for evaluating the gradients.\n",
        "# y = np.random.normal(0,1,size=(D_o,n_samples))\n",
        "# y = np.random.normal(0,1,size=(n_samples, D_o))\n",
        "y = np.eye(D_o)[np.random.choice(D_o, n_samples)]\n",
        "L = compute_cost(net_output, y)\n",
        "print(\"y = \",y)\n",
        "print(\"Cost = %3.3f\"%(L))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98WmyqFYWA-0",
        "tags": []
      },
      "source": [
        "Now let's compute the derivatives of the network.  We already computed the forward pass.  Let's compute the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LJng7WpRPLMz",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# We'll need the indicator function in the backwards pass, since this is the derivative of the ReLU function.\n",
        "def indicator_function(x):\n",
        "    x_in = np.array(x)\n",
        "    x_in[x_in>0] = 1\n",
        "    x_in[x_in<=0] = 0\n",
        "    return x_in\n",
        "\n",
        "# Main backward pass routine\n",
        "def backward_pass(all_weights, all_biases, all_f, all_h, y):\n",
        "    # We'll store the derivatives dl_dweights and dl_dbiases in lists as well\n",
        "    all_dl_dweights = [None] * (K+1)\n",
        "    all_dl_dbiases = [None] * (K+1)\n",
        "    # And we'll store the derivatives of the cost with respect to the activation and preactivations in lists\n",
        "    all_dl_df = [None] * (K+1)\n",
        "    all_dl_dh = [None] * (K+1)\n",
        "    # Again for convenience we'll stick with the convention that all_h[0] is the net input and all_f[k] in the net output\n",
        "\n",
        "    # Compute derivatives of the cost with respect to the network output\n",
        "    all_dl_df[K] = np.array(d_cost_d_output(all_f[K],y))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Now work backwards through the network from layer K to layer 0\n",
        "    # TODO Change the range to the correct order in which the layers should be procesed.\n",
        "    # layer_range = range(K+1)\n",
        "    layer_range = range(K,-1,-1)\n",
        "\n",
        "    for layer in layer_range:\n",
        "        # TODO Calculate the derivatives of the cost with respect to the biases at layer from all_dl_df[layer]. (eq 7.21, pdf version: eq 7.22)\n",
        "        # NOTE!  To take a copy of matrix X, use Z=np.array(X)\n",
        "        # REPLACE THIS LINE\n",
        "        # all_dl_dbiases[layer] = np.zeros_like(all_biases[layer])\n",
        "        ############\n",
        "        # all_dl_dbiases[layer] = np.sum(all_dl_df[layer], axis=1, keepdims=True)\n",
        "        all_dl_dbiases[layer] = np.sum(all_dl_df[layer], axis=0, keepdims=True).T\n",
        "\n",
        "        # TODO Calculate the derivatives of the cost with respect to the weights at layer from all_dl_df[layer] and all_h[layer] See Prep exercise 2.5 and also eq 7.22 in the book, (pdf version: eq 7.23)\n",
        "        # Don't forget to use np.matmul\n",
        "        # REPLACE THIS LINE\n",
        "        # all_dl_dweights[layer] = np.zeros_like(all_weights[layer])\n",
        "        # all_dl_dweights[layer] = all_dl_df[layer] @ all_h[layer].T\n",
        "        # all_dl_dweights[layer] = (all_dl_df[layer] @ all_h[layer].T)\n",
        "        all_dl_dweights[layer] = all_dl_df[layer].T @ all_h[layer]\n",
        "\n",
        "        # TODO: calculate the derivatives of the cost with respect to the activations from weight and derivatives of next preactivations (second part of last line of eq 7.24, pdf version: eq 7.25)\n",
        "        # REPLACE THIS LINE\n",
        "        # all_dl_dh[layer] = np.zeros_like(all_h[layer])\n",
        "        # all_dl_dh[layer] = all_weights[layer].T @ all_dl_df[layer]\n",
        "\n",
        "        all_dl_dh[layer] = all_dl_df[layer] @ all_weights[layer]\n",
        "\n",
        "        if layer > 0:\n",
        "            # TODO Calculate the derivatives of the cost with respect to the pre-activation f (use derivative of ReLu function, first part of last line of eq 7.24, pdf version: eq 7.25)\n",
        "            # REPLACE THIS LINE\n",
        "            # all_dl_df[layer-1] = np.zeros_like(all_f[layer-1])\n",
        "            all_dl_df[layer-1] = all_dl_dh[layer] * indicator_function(all_f[layer-1])\n",
        "\n",
        "\n",
        "    return all_dl_dweights, all_dl_dbiases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9A9MHc4sQvbp",
        "tags": []
      },
      "outputs": [],
      "source": [
        "all_dl_dweights, all_dl_dbiases = backward_pass(all_weights, all_biases, all_f, all_h, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK-UtE3hreAK",
        "outputId": "02409154-8cca-4c6d-aa9d-cc62b1d2b5a0",
        "scrolled": false,
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------\n",
            "Bias 0, derivatives from backprop:\n",
            "[[-16.779]\n",
            " [  0.   ]\n",
            " [-18.063]\n",
            " [  5.91 ]\n",
            " [ 32.004]\n",
            " [ -5.548]]\n",
            "Bias 0, derivatives from finite differences\n",
            "[[-0.624]\n",
            " [ 0.   ]\n",
            " [ 0.057]\n",
            " [ 1.177]\n",
            " [ 0.535]\n",
            " [-0.109]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Bias 1, derivatives from backprop:\n",
            "[[ 7.903]\n",
            " [10.528]\n",
            " [ 0.   ]\n",
            " [ 5.321]\n",
            " [ 0.   ]\n",
            " [ 0.   ]]\n",
            "Bias 1, derivatives from finite differences\n",
            "[[ 0.264]\n",
            " [ 0.16 ]\n",
            " [ 0.   ]\n",
            " [-0.134]\n",
            " [ 0.   ]\n",
            " [ 0.   ]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Bias 2, derivatives from backprop:\n",
            "[[-0.928]\n",
            " [11.   ]\n",
            " [ 9.724]\n",
            " [ 3.953]\n",
            " [ 5.69 ]\n",
            " [-0.12 ]]\n",
            "Bias 2, derivatives from finite differences\n",
            "[[-0.163]\n",
            " [-0.004]\n",
            " [ 0.329]\n",
            " [-0.156]\n",
            " [ 0.234]\n",
            " [ 0.131]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Bias 3, derivatives from backprop:\n",
            "[[11.793]\n",
            " [-5.411]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.   ]]\n",
            "Bias 3, derivatives from finite differences\n",
            "[[0.142]\n",
            " [0.132]\n",
            " [0.   ]\n",
            " [0.   ]\n",
            " [0.   ]\n",
            " [0.   ]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Bias 4, derivatives from backprop:\n",
            "[[0.   ]\n",
            " [1.087]\n",
            " [4.261]\n",
            " [0.   ]\n",
            " [0.   ]\n",
            " [6.168]]\n",
            "Bias 4, derivatives from finite differences\n",
            "[[ 0.   ]\n",
            " [ 0.124]\n",
            " [ 0.256]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [-0.028]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Bias 5, derivatives from backprop:\n",
            "[[ 3.167]\n",
            " [-1.167]\n",
            " [-2.   ]]\n",
            "Bias 5, derivatives from finite differences\n",
            "[[-0.183]\n",
            " [-0.617]\n",
            " [-0.2  ]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Weight 0, derivatives from backprop:\n",
            "[[-22.831]\n",
            " [  0.   ]\n",
            " [-22.732]\n",
            " [ 14.658]\n",
            " [ 28.458]\n",
            " [ -0.112]]\n",
            "Weight 0, derivatives from finite differences\n",
            "[[-0.605]\n",
            " [ 0.   ]\n",
            " [-0.02 ]\n",
            " [ 1.074]\n",
            " [ 0.005]\n",
            " [ 0.207]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Weight 1, derivatives from backprop:\n",
            "[[44.773  0.    15.25  32.608 40.523  0.   ]\n",
            " [25.268  0.     2.115  4.257 17.59  14.709]\n",
            " [ 0.     0.     0.     0.     0.     0.   ]\n",
            " [46.431  0.    19.021 39.481 44.741 -8.22 ]\n",
            " [ 0.     0.     0.     0.     0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.     0.   ]]\n",
            "Weight 1, derivatives from finite differences\n",
            "[[ 1.415  0.     0.464  0.985  1.268  0.   ]\n",
            " [-0.354  0.    -0.321 -0.586 -0.514  0.632]\n",
            " [ 0.     0.     0.     0.     0.     0.   ]\n",
            " [ 0.545  0.     0.461  0.881  0.719 -0.602]\n",
            " [ 0.     0.     0.     0.     0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.     0.   ]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Weight 2, derivatives from backprop:\n",
            "[[ 0.     0.     0.    -1.403  0.     0.   ]\n",
            " [ 3.829 -0.774  0.    33.903  0.     0.   ]\n",
            " [ 3.194  0.     0.    29.412  0.     0.   ]\n",
            " [ 2.124 -0.509  0.    12.943  0.     0.   ]\n",
            " [ 0.     9.842  0.    14.583  0.     0.   ]\n",
            " [-1.029  3.395  0.    -0.816  0.     0.   ]]\n",
            "Weight 2, derivatives from finite differences\n",
            "[[ 0.     0.     0.    -0.247  0.     0.   ]\n",
            " [ 0.057 -0.35   0.    -0.084  0.     0.   ]\n",
            " [ 0.038  0.     0.     0.896  0.     0.   ]\n",
            " [ 0.027 -0.218  0.    -0.408  0.     0.   ]\n",
            " [ 0.     0.366  0.     0.547  0.     0.   ]\n",
            " [-0.026  0.229  0.     0.45   0.     0.   ]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Weight 3, derivatives from backprop:\n",
            "[[  0.253  47.279   5.993   1.964   0.293  84.713]\n",
            " [ -0.275 -21.652  -4.104  -0.53    2.646 -45.145]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]]\n",
            "Weight 3, derivatives from finite differences\n",
            "[[ 0.028  0.464  0.255  0.005  0.021  1.043]\n",
            " [-0.05   0.745 -0.17   0.036  0.11   0.793]\n",
            " [ 0.     0.     0.     0.     0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.     0.   ]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Weight 4, derivatives from backprop:\n",
            "[[ 0.     0.     0.     0.     0.     0.   ]\n",
            " [ 2.573  2.282  0.     0.     0.     0.   ]\n",
            " [12.359  8.686  0.     0.     0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.     0.   ]\n",
            " [28.739 11.265  0.     0.     0.     0.   ]]\n",
            "Weight 4, derivatives from finite differences\n",
            "[[0.    0.    0.    0.    0.    0.   ]\n",
            " [0.428 0.232 0.    0.    0.    0.   ]\n",
            " [0.917 0.492 0.    0.    0.    0.   ]\n",
            " [0.    0.    0.    0.    0.    0.   ]\n",
            " [0.    0.    0.    0.    0.    0.   ]\n",
            " [0.022 0.031 0.    0.    0.    0.   ]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "Weight 5, derivatives from backprop:\n",
            "[[  0.      9.603  12.407   0.      0.     15.011]\n",
            " [  0.     -6.169  -5.174   0.      0.    -10.184]\n",
            " [  0.     -3.434  -7.234   0.      0.     -4.828]]\n",
            "Weight 5, derivatives from finite differences\n",
            "[[ 0.    -0.333 -0.583  0.     0.    -0.436]\n",
            " [ 0.    -0.973 -2.213  0.     0.    -1.364]\n",
            " [ 0.    -0.343 -0.723  0.     0.    -0.483]]\n",
            "Failure!  Derivatives different.\n",
            "-----------------------------------------------\n",
            "SUMMARY\n",
            "Bias:  0 / 6 derivatives match:\n",
            "Weight:  0 / 6 derivatives match:\n",
            "Failure!  All derivatives did not match.\n"
          ]
        }
      ],
      "source": [
        "np.set_printoptions(precision=3)\n",
        "# Make space for derivatives computed by finite differences\n",
        "all_dl_dweights_fd = [None] * (K+1)\n",
        "all_dl_dbiases_fd = [None] * (K+1)\n",
        "\n",
        "# Let's test if we have the derivatives right using finite differences\n",
        "delta_fd = 0.000001\n",
        "\n",
        "# Count number of successes\n",
        "bias_success = 0\n",
        "weight_success = 0\n",
        "\n",
        "# Test the dervatives of the bias vectors\n",
        "for layer in range(K+1):\n",
        "    dl_dbias  = np.zeros_like(all_biases[layer])\n",
        "    # For every element in the bias\n",
        "    for row in range(all_biases[layer].shape[0]):\n",
        "        # Take copy of biases.  We'll change one element each time\n",
        "        all_biases_copy = [np.array(x) for x in all_biases]\n",
        "        all_biases_copy[layer][row] += delta_fd\n",
        "        network_output_1, *_ = forward_pass(net_input, all_weights, all_biases_copy)\n",
        "        network_output_2, *_ = forward_pass(net_input, all_weights, all_biases)\n",
        "        dl_dbias[row] = (compute_cost(network_output_1, y) - compute_cost(network_output_2,y))/delta_fd\n",
        "    all_dl_dbiases_fd[layer] = np.array(dl_dbias)\n",
        "    print(\"-----------------------------------------------\")\n",
        "    print(\"Bias %d, derivatives from backprop:\"%(layer))\n",
        "    print(all_dl_dbiases[layer])\n",
        "    print(\"Bias %d, derivatives from finite differences\"%(layer))\n",
        "    print(all_dl_dbiases_fd[layer])\n",
        "    if np.allclose(all_dl_dbiases_fd[layer],all_dl_dbiases[layer],rtol=1e-03, atol=1e-07, equal_nan=False):\n",
        "        print(\"Success!  Derivatives match.\")\n",
        "        bias_success+=1\n",
        "    else:\n",
        "        print(\"Failure!  Derivatives different.\")\n",
        "\n",
        "\n",
        "\n",
        "# Test the derivatives of the weights matrices\n",
        "for layer in range(K+1):\n",
        "    dl_dweight  = np.zeros_like(all_weights[layer])\n",
        "    # For every element in the bias\n",
        "    for row in range(all_weights[layer].shape[0]):\n",
        "        for col in range(all_weights[layer].shape[1]):\n",
        "            # Take copy of biases. We'll change one element each time\n",
        "            all_weights_copy = [np.array(x) for x in all_weights]\n",
        "            all_weights_copy[layer][row][col] += delta_fd\n",
        "            network_output_1, *_ = forward_pass(net_input, all_weights_copy, all_biases)\n",
        "            network_output_2, *_ = forward_pass(net_input, all_weights, all_biases)\n",
        "            dl_dweight[row][col] = (compute_cost(network_output_1, y) - compute_cost(network_output_2,y))/delta_fd\n",
        "    all_dl_dweights_fd[layer] = np.array(dl_dweight)\n",
        "    print(\"-----------------------------------------------\")\n",
        "    print(\"Weight %d, derivatives from backprop:\"%(layer))\n",
        "    print(all_dl_dweights[layer])\n",
        "    print(\"Weight %d, derivatives from finite differences\"%(layer))\n",
        "    print(all_dl_dweights_fd[layer])\n",
        "    if np.allclose(all_dl_dweights_fd[layer],all_dl_dweights[layer],rtol=1e-03, atol=1e-07, equal_nan=False):\n",
        "        print(\"Success!  Derivatives match.\")\n",
        "        weight_success+=1\n",
        "    else:\n",
        "        print(\"Failure!  Derivatives different.\")\n",
        "\n",
        "print(\"-----------------------------------------------\")\n",
        "\n",
        "print(\"SUMMARY\")\n",
        "print(\"Bias:  %d / %d derivatives match:\"%(bias_success,K+1))\n",
        "print(\"Weight:  %d / %d derivatives match:\"%(weight_success,K+1))\n",
        "if bias_success+weight_success == 2*(K+1):\n",
        "    print(\"Success!  All derivatives match.\")\n",
        "else:\n",
        "    print(\"Failure!  All derivatives did not match.\")"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "celltoolbar": "Tags",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}